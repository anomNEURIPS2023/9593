defaults:
  - _self_

agent_name: sac
#env: cheetah_run
#env_type: dm_control

#env: HalfCheetah-v2
#env_type: gym

# TODO: ONLY FOR METAWORLD
# metaworld
#env: door-open-v2
#env_type: metaworld
metaworld_max_steps: 150

# multitask envs
env: halfcheetah_forward
env_type: multitask





# this needs to be specified manually
experiment: regular # medium_expert

num_train_steps: 1e6
replay_buffer_capacity: ${num_train_steps}

num_seed_steps: 5000

eval_frequency: 5000
num_eval_episodes: 10

device: cuda

# logger
log_frequency: 5000
log_save_tb: true

# video recorder
save_video: false

# network:
hidden_dim: 1024
batch_size: 1024

seed: 0
lr: 0.001
init_temp: 0.1



agent:
  _target_: agent.sac.SACAgent
  obs_dim: ???
  action_dim: ???
  action_range: ???
  device: ${device}
  critic_cfg: ${double_q_critic}
  actor_cfg: ${diag_gaussian_actor}
  discount: 0.99
  init_temperature: ${init_temp} #0.1
  alpha_lr:  ${lr} #0.0001
  alpha_betas: [0.9, 0.999]
  actor_lr: ${lr} #0.0001
  actor_betas: [0.9, 0.999]
  actor_update_frequency: 1
  critic_lr: ${lr} #0.0001
  critic_betas: [0.9, 0.999]
  critic_tau: 0.005
  critic_target_update_frequency: 2
  batch_size: ${batch_size}
  learnable_temperature: True



double_q_critic:
  _target_: agent.critic.DoubleQCritic
  obs_dim: ${agent.obs_dim}
  action_dim: ${agent.action_dim}
  hidden_dim: ${hidden_dim}
  hidden_depth: 2

diag_gaussian_actor:
  _target_: agent.actor.DiagGaussianActor
  obs_dim: ${agent.obs_dim}
  action_dim: ${agent.action_dim}
  hidden_dim: ${hidden_dim}
  hidden_depth: 2
  log_std_bounds: [-10, 2]

# hydra configuration
hydra:
  run:
    dir: ./runs/